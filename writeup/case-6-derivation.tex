
\subsection*{5. Generalization allowing for $k$ subgroups.}

Let $\Var(\gamma_i) = v_{i}$ and assume $\Cov(\gamma_i, \gamma_j) = 0$ for $i \neq j$

we saw from before that we can write

\begin{align*}
	E J_i &= p_1 ( b_1 - c_1 (\bar{y}_1 b_1 + \bar{y}_2 b_2 + \ldots + \bar{y}_k b_k ))^2 \\
		  &+ p_2 ( b_2 - c_2 (\bar{y}_1 b_1 +  \bar{y}_2 b_2 + \ldots + \bar{y}_k b_k ))^2 \\
		  &\vdots \\
		  &+ p_k ( b_k - c_k (\bar{y}_1 b_1 +\bar{y}_2 b_2 +  \ldots + \bar{y}_k b_k ))^2 \\
\end{align*}

expanding $c_i$ 

We can describe these as the loss for an individual $i$ in subgroup $k$ 

\begin{align*}
		  &W_1^2= \left( b_1 -  \bar{y}_1 v_1\frac{ \bar{y}_1 b_1 + \bar{y}_2 b_2 + \ldots + \bar{y}_k b_k }{\bar{y}_1^2 v_1 + \bar{y}_2^2 v_2 + \ldots + \bar{y}_k^2 v_k} \right)^2 \\
		  &W_2^2= \left( b_2 -  \bar{y}_2 v_2\frac{ \bar{y}_1 b_1 + \bar{y}_2 b_2 + \ldots + \bar{y}_k b_k }{\bar{y}_1^2 v_1 + \bar{y}_2^2 v_2 + \ldots + \bar{y}_k^2 v_k} \right)^2 \\
		  & \vdots \\
		  &W_k^2 = \left( b_3 - \bar{y}_k v_k\frac{ \bar{y}_1 b_1 + \bar{y}_2 b_2 + \ldots + \bar{y}_k b_k }{\bar{y}_1^2 v_1 + \bar{y}_2^2 v_2 + \ldots + \bar{y}_k^2 v_k} \right)^2 \\
\end{align*} 

where $\bar{y}_k = (1 - \bar{x}_1 - \ldots - \bar{y}_{k-1}$)

Call $\xi = \frac{ \bar{y}_1 b_1 + \bar{y}_2 b_2 + \ldots + \bar{y}_k b_k }{\bar{y}_1^2 v_1 + \bar{y}_2^2 v_2 + \ldots + \bar{y}_k^2 v_k}$


\begin{align*}
		  &W_1^2=  \left( b_1 - \bar{y}_1 v_1 \xi \right)^2 = b_1^2 - 2 b_1 v_1 \bar{y}_1 \xi + \bar{y}_1^2 v_1^2 \xi^2 \\
		  &W_2^2= \left( b_2 -  \bar{y}_2 v_2 \xi \right)^2 = b_2^2 - 2 b_2 v_2 \bar{y}_2 \xi + \bar{y}_2^2 v_2^2 \xi^2 \\
		  & \vdots \\
		  &W_k^2 = \left( b_3 - \bar{y}_k v_k \xi \right)^2 = b_k^2 - 2 b_k v_k \bar{y}_k \xi + \bar{y}_k^2 v_k^2 \xi^2 \\
		  % & = \left( b_3 - \bar{y}_k \xi \right)^2 = b_3^2 - 2 b_3 \xi(\bar{y}_1^2 + 2 \bar{y}_1 \bar{y}_2 - 2 \bar{y}_1 + \bar{y}_2^2 - 2 \bar{y}_2 +1)  + \bar{y}_3^2 \xi^2 \\
\end{align*} 

taking expectation over $\gamma_i$ we get

\begin{align*}
		  &W_1^2=  v_1 - 2 v_1^2 \frac{\bar{y}_1^2 }{\bar{y}_1^2 v_1 + \bar{y}_2^2 v_2 + \ldots + \bar{y}_k^2 v_k} + \bar{y}_1^2 v_1^2 \xi^2 \\
		  &W_2^2=  v_2 - 2 v_2^2 \frac{\bar{y}_2^2 }{\bar{y}_1^2 v_1 + \bar{y}_2^2 v_2 + \ldots + \bar{y}_k^2 v_k} + \bar{y}_2^2 v_2^2 \xi^2 \\
		  & \vdots \\         ^2
		  &W_k^2=  v_k - 2 v_k^2 \frac{\bar{y}_k^2 }{\bar{y}_1^2 v_1 + \bar{y}_2^2 v_2 + \ldots + \bar{y}_k^2 v_k} + \bar{y}_k^2 v_k^2 \xi^2 \\
\end{align*}

% and start by assuming $\Var(\gamma_i) = 1$ then

\begin{align*}
	MSE = &p_1 W_1^2 + p_2 W_2^2 + \ldots + p_k W_k^2 \\
	MSE = &p_1[v_1 - 2 \frac{\bar{y}_1^2 v_1^2}{\bar{y}_1^2 v_1 + \bar{y}_2^2 v_2 + \ldots + \bar{y}_k^2 v_k} + \bar{y}_1^2 v_1^2 \xi^2] \\
	    + &p_2[v_2 - 2 \frac{\bar{y}_2^2 v_2^2}{\bar{y}_1^2 v_1 + \bar{y}_2^2 v_2 + \ldots + \bar{y}_k^2 v_k} + \bar{y}_2^2 v_2^2 \xi^2 ] \\ 
	      & \vdots \\
	    + &p_k [v_k -2 \frac{\bar{y}_k^2 v_k^2}{\bar{y}_1^2 v_1 + \bar{y}_2^2 v_2 + \ldots + \bar{y}_k^2 v_k} + \bar{y}_k^2 v_k^2 \xi^2] \\
	MSE = &\frac{\bar{y}_1^2 v_1 (\sum_{i=2}^k v_i p_i) + \bar{y}_2^2 v_2 (\sum_{i \neq2}^k v_i p_i)+ \ldots + \bar{y}_k^2 v_k (\sum_{i=1}^{k-1} v_i p_i)}{\bar{y}_1^2 v_1 + \bar{y}_2^2 v_2 + \ldots + \bar{y}_k^2 v_k} ] \\
\end{align*}

First, assume $p = p_1 = p_2 = \dots = p_k = \frac{1}{k}$
            
and note that in expectation (assuming $Cov(\gamma_i, \gamma_j) = 0$ for $i \neq j$)
\begin{align*}
	(\bar{y}_1 b_1 + \bar{y}_2 b_2 + \dots + \bar{y}_k b_3 )^2 &= \Var(\gamma_1) \bar{y}_1^2 + \Var(\gamma_2) \bar{y}_2^2 + \dots +\Var(\gamma_3) \bar{y}_k^2 \\
\end{align*}

then if $v = v_1 = v_2 = \dots = v_k$

\begin{align*}
	MSE = & p v  [k - 2 + 1] \\
	= &p v \frac{k-1}{k}
\end{align*} 
