% {{{
\input{$HOME/.config/nvim/snippets/.math.preamble.tex}
% }}}

% Title {{{
\begin{document}

\begin{center}
	{\large \bf Jason Abaluck }   \\ \large optimal Representation \\ Ephraim Sutherland
\end{center}
% }}}

\tableofcontents


% First we'll start by examing a simple scenario where two subgroups have equal variance (e.g. considermen and women).


\subsection*{Setup}


\begin{enumerate}

	\item  Suppose a physician can only see see ATE and some measure of representativeness. They have prior
		$\bar{\beta}$ and  $\beta_{ATE} = (1/N) \sum \beta_{i}$.

	\item need model for betas related to each other based on $x$'s.
		WLOG, suppose
		\begin{align*}
			\beta(x_i) = x_i \gamma
		\end{align*} 
		Where $x_i$ is a vector of characteristics and $\gamma$ is a vector of coefficients. \\
		If you know $\gamma$, then you know $\beta$ for any given patient.
	\item However, you don't observe $\gamma$, you instead observe:
		$\beta_{ATE} = \bar x \gamma$ where $\bar x = (\frac{1}{N}) \sum x_i$
	\item We know $\beta_i$ for patients with characteristics $\bar x$ (it is $\beta_{ATE}$).
	\item For other patients, need to solve
		\begin{align*}
			\beta_{i,post} = \E(x_i \gamma | \bar x \gamma = \beta_{ATE})
		\end{align*} 

	\item to solve
		\begin{enumerate}
			\item
				\begin{align*}
					\beta_{i,post} & = \E (x_i \gamma | \bar x \gamma = \beta_{ATE})                                                                        \\
								   & = \E((x_i - c_i \bar{x}) \gamma | \bar{x} \gamma = \beta_{ATE}) + c_i \E(\bar{x} \gamma | \bar{x} \gamma = \beta_{ATE}) \\
								   & = \E((x_{i} - c_i \bar{x}) \gamma | \bar{x} \gamma = \beta_{ATE}) + c_i \beta_{ATE}                                       \\
				\end{align*}
				For any constant $c_i$. \\
				Choose $c_i$ so that 
				\begin{align*}
					\Cov((x_{i} - c_i \bar{x})\gamma, \bar{x} \gamma) = 0
				\end{align*} 
				maybe assume normality so that this guarantees independence.
				Then,
				\begin{align*}
					\E((x_i - c_i \bar{x}) \gamma | \bar{x} \gamma = \beta_{ATE}) = (x_{i} - c_i \bar{x}) \E(\gamma)
				\end{align*}
				So then
				\begin{align*}
					(x_i - c_i \bar{x}) \E(\gamma) + c_i \beta_{ATE} = x_i \E(\gamma) + c_i (\beta_{ATE} - \bar{x} \E(\gamma))
				\end{align*} 
				($c_i$ depends on $x_i$)

				In other words, your belief is your prior, adjusted based on the difference between the observed ATE and your prior about the ATE.
				The key question is how much adjustment you do which depends on $"c_i"$. We choose $c_i$ to solve:

				\begin{align*}
                          & \Cov ((x_i - c_i \bar{x}) \gamma, \bar{x} \gamma) = 0                          \\
				 \iff     & \Cov(x_i \gamma, \bar{x} \gamma) -c_i \Cov(\bar{x} \gamma, \bar{x} \gamma) = 0 \\
					\iff  & \Cov(x_i \gamma, \bar{x} \gamma) = c_i \Var(\bar{x} \gamma)                  \\
				\iff & c_i = \frac{\Cov(\beta_i, \beta_{ATE}) }{ \Var(\beta_{ATE})}
				\end{align*} 
				The random variable in this context is $\gamma$ (the coefficients on the $x $'s) in this case $\Var(\beta_{ATE})$ is a measure of how uncertain one was about what $\beta_{ATE}$ would be before doing the trial.

				$c_i$ is the equation for a regression of $\beta_i$ on $\beta_{ATE}$. In other words, we take a bunch of patients with characteristics $x_i$ and we keep redrawing the gammas from our prior distribution the we ask how correlated  $\beta_i$ and $\beta_{ATE}$ are. If they are more correlated (as they would be for patients where the $x_i$ are closer to $\bar{x}$ we update more.

				To compute $c_i$, we just need to know $x_i$ $\bar{x}$, and the distribution of $\gamma$.

				Suppose we want to design the trial to minimize:
				\begin{align*}
					\min \E[(\beta_i - \beta_{i,post})^2]
				\end{align*} 
		\end{enumerate}

\end{enumerate} 

\subsection*{Simple Cases}
\begin{enumerate}
	\item There is just one $x$ and it is binary (old v young). Can it be solved analytically?
	\item Can you solve a 2-dimensional case? 
\end{enumerate}


First observe that in our current setup, $c$ does not depend on the $\gamma$'s

We have that
\begin{align*}
	c = \frac{\Cov (x \gamma, \bar{x} \gamma)}{ \Var( \bar{x} \gamma )}
\end{align*} 

so for individual i, $c$ reduces to


\begin{align*}
	c &= \frac{1^2\Var(\gamma_0) + x \bar{x} \Var(\gamma_1)}{1^2\Var(\gamma_0) + \bar{x}^2\Var(\gamma_1)} \quad \text{because $\Var(\gamma_0) = \Var(\gamma_1) = \Var(\gamma) = 1$} \\
	  &= \frac{(1 + x \bar{x})\Var(\gamma)}{(1 + \bar{x}^2)\Var(\gamma)} = \frac{\vec{x}' \vec{\bar{x}}}{\vec{\bar{x}}' \vec{\bar{x}}}
\end{align*} 




\section{Appendix}

\begin{center}
	{\large \bf Derivations }
\end{center}

\subsection*{1. symmetric case}

Recall we want to minimize
\begin{align*}
\min \E_x \left(\E_{\gamma_{0,1}}[(\beta_i - \beta_{i,post})^2]\right)
\end{align*}

One way we can rewrite these equations is as the effect of women vs men.
let the squared error (SE) be
\begin{align*}
	\sqrt{SE} &= \beta_i -  \beta_{i,post} = \underbrace{[(1-x) \gamma_0 + x \gamma_1]}_{\text{$\beta_i$}}  - \underbrace{[((\vec{x} - c \vec{\bar{x}})\E(\gamma) + c \beta_{ATE})]}_\text{$\beta_{i,post}$} \\
	  &= [(1-x) \gamma_0 + x \gamma_1]  - [((1-x) - c (1 - \bar{x})) \E(\gamma_0) + (x - c \bar{x})\E(\gamma_1) + c((1-\bar{x}) \gamma_0 + \bar{x} \gamma_1)] \\
	  % &= [(1-x) \gamma_0 + x \gamma_1  - (x - c \bar{x})\E(\gamma) + c((1-\bar{x}) \gamma_0 + \bar{x} \gamma_1)] \\
\end{align*}

$\beta_i$ $\beta_{i, post}$ and as a result, $SE$ are all a function of $x$. We can then describe

$\beta_{i}^{men} = \beta_i (x=1)$ and similarly for other terms to get

So 
\begin{align*}
	\beta_i^{men} &= \gamma_1 \\
	\beta_i^{post,men} &= -c_{men} ( 1- \bar{x}) \bar{\gamma_0} + (1 - c_{men}\bar{x}) \bar{\gamma_1} + c_{men} [ (1-\bar{x}) \gamma_0 + \bar{x}\gamma_1] \\
	\beta_i^{women} &= \gamma_0 \\
	\beta_i^{post,women} &= [ 1 - c_{wom}(1-\bar{x}) ]\E(\gamma_0) - c_{wom} \bar{x} \E(\gamma_1) + c_{wom} (1- \bar{x}) \gamma_0 + c_{wom} \bar{x} \gamma_1 \\
\end{align*} 

this can be broken down into

\begin{align*}
	\sqrt{ SE } &= (\beta_i^{men} - \beta_i^{post, men} )+ (\beta_i^{wom} - \beta_i^{post, wom}) \\
\end{align*} 

let 
 \begin{align*}
	 W^{men} &=  (\beta_i^{men} - \beta_i^{post, men}) \\
			&= (1 - \bar{x} c_{men}) (\gamma_1 - \bar{\gamma_1}) - c_{men} ( 1 - \bar{x}) (\gamma_0 - \bar{\gamma_0}) \\
			&= (c_{wom} (1 - \bar{x})) (\gamma_1 - \bar{\gamma_1}) - c_{men} ( 1 - \bar{x}) (\gamma_0 - \bar{\gamma_0}) \\
	 W^{wom} &=  (\beta_i^{wom} - \beta_i^{post, wom}) \\
			 &= [ 1 - c_{wom}(1 - \bar{x}) ] (\gamma_0 - \bar{\gamma_0}) - c_{wom} \bar{x} (\gamma_1 - \bar{\gamma_1}) \\
			 &= [ c_{men} \bar{x}] (\gamma_0 - \bar{\gamma_0}) - c_{wom} \bar{x} (\gamma_1 - \bar{\gamma_1}) \\
\end{align*} 
and recall that
\begin{align*}
	c_{i} &= \frac{(1-x)(1-\bar{x})\Var(\gamma_0) + x\bar{x}\Var(\gamma_1)}{(1-\bar{x})^2\Var(\gamma_0) + \bar{x}^2\Var(\gamma_1)} \\
		  % &= \frac{(1-x)(1-\bar{x}) + x\bar{x}}{(1-\bar{x})^2 + \bar{x}^2} 
\end{align*}

so if you are a man, then 

\begin{align*}
	c_{men} =  \frac{\bar{x}\Var(\gamma_1)}{(1-\bar{x})^2\Var(\gamma_0) + \bar{x}^2\Var(\gamma_1)}
\end{align*}

and likewise if you are a woman, then

\begin{align*}
	c_{woman} = \frac{(1-\bar{x})\Var(\gamma_0)}{(1-\bar{x})^2\Var(\gamma_0) + \bar{x}^2\Var(\gamma_1)}
\end{align*}


% and 
% \begin{align*}
% 	W_{men}^2 &= \left(\frac{(1-\bar{x})^4}{((1-\bar{x})^2 + \bar{x}^2)^2}\right) \Var(\gamma_1) +  \frac{\bar{x}^2 (1-\bar{x})^2}{((1-\bar{x})^2 + \bar{x}^2)^2} \Var(\gamma_0) \\
% 			  &= \big(\frac{(1-\bar{x})^2 \Var(\gamma_1)} {(1-\bar{x})^2 + \bar{x}^2} \\
% 	W_{women}^2 &= \left(\frac{(\bar{x})^4}{((1-\bar{x})^2 + \bar{x}^2)^2}\right) \Var(\gamma_0) + \frac{\bar{x}^2 (1-\bar{x})^2}{((1-\bar{x})^2 + \bar{x}^2)^2} \Var(\gamma_1) \\
% 				&= \frac{(\bar{x})^2 \Var(\gamma_0) }{(1-\bar{x})^2 + \bar{x}^2} \\
% \end{align*} 


% so then taking the mean ($E_{x}$) with respect to $x$, we can write the mean squared error (MSE) as
% \begin{align*}
% 	MSE &= p W_{men}^2 + (1 - p)W_{wom}^2 \\
% 	&= p \left(\frac{(1-\bar{x})^4\Var(\gamma_1) + ( \bar{x}^2 (1-\bar{x})^2 ) \Var(\gamma_0)}{((1-\bar{x})^2 + \bar{x}^2)^2}\right) + (1-p)\left(\frac{(\bar{x})^4 \Var(\gamma_0) + ( \bar{x}^2 (1-\bar{x})^2 ) \Var(\gamma_1) }{((1-\bar{x})^2 + \bar{x}^2)^2}\right) \\
% \end{align*} 

% so using the fact that $\Var(\gamma_0) = \Var(\gamma_1) = 1$ we get

% \begin{align*}
% 	 MSE &= p \left(\frac{(1-\bar{x})^4 + ( \bar{x}^2 (1-\bar{x})^2 ) }{((1-\bar{x})^2 + \bar{x}^2)^2}\right) + (1-p)\left(\frac{(\bar{x})^4  + ( \bar{x}^2 (1-\bar{x})^2 ) }{((1-\bar{x})^2 + \bar{x}^2)^2}\right) \\
% 	 MSE &= p \left(\frac{(1-\bar{x})^2 }{(1-\bar{x})^2 + \bar{x}^2}\right) + (1-p)\left(\frac{(\bar{x})^2 }{(1-\bar{x})^2 + \bar{x}^2}\right) \\
% 			 &= p \frac{(1-\bar{x})^2}{(1-\bar{x})^2 + \bar{x}^2} + (1-p) \frac{\bar{x}^2}{(1-\bar{x})^2 + \bar{x}^2}
% \end{align*} 

% In other words for individual $i$ we have that

% \begin{align*}
% 	 MSE &= p (1 - c_{men} \bar{x}) + (1-p) (1 - c_{women} (1 - \bar{x})) \\
% 	 MSE &= p \left(1 - \frac{\bar{x}^2}{(1-\bar{x})^2 + \bar{x}^2}\right) + (1-p) \left(1 - \frac{(1-\bar{x})^2}{(1-\bar{x})^2 + \bar{x}^2} \right) \\
% 	 MSE &= p \left(\frac{(1 - \bar{x})^2}{(1-\bar{x})^2 + \bar{x}^2}\right) + (1-p) \left(\frac{\bar{x}^2}{(1-\bar{x})^2 + \bar{x}^2} \right) \\
% 	 MSE &= p \left[ (1 - \bar{x})c_{wom} \right] + (1-p) \left[ \bar{x}c_{men} \right] \\
% \end{align*} 

% Interestingly this implies a FOC of

% \begin{align*}
% 	\frac{\partial MSE }{\partial \bar{x}} &= p \left(\frac{2 \bar{x} (1 - \bar{x})}{((1-\bar{x})^2 + \bar{x}^2)^2}\right) - (1-p) \left(\frac{2 \bar{x} (1 - \bar{x})}{((1-\bar{x})^2 + \bar{x}^2)^2} \right) = 0 \\
% 	\frac{\partial MSE}{\partial \bar{x}} &= (2p -1) \left(\frac{2 \bar{x} (1 - \bar{x})}{((1-\bar{x})^2 + \bar{x}^2)^2}\right) = 0
% \end{align*} 

% meaning we've retrieved our original FOC.



And, allowing for arbitrary variances, we can say that

\begin{align*}
	W_{men}^2 &= \left(\frac{(1 - \bar{x})^4}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2}\right)\Var(\gamma_0)^2 \Var(\gamma_1)  \\
			  & -  \frac{2 (1 - \bar{x})^2[\bar{x} (1-\bar{x})]}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2} \Var(\gamma_1) \Var(\gamma_0) \Cov(\gamma_0, \gamma_1)  \\
			  & + \frac{\bar{x}^2 (1-\bar{x})^2}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2} \Var(\gamma_1)^2 \Var(\gamma_0) \\
			  &= \Var(\gamma_0) \Var(\gamma_1) \bigg(\frac{(1 - \bar{x})^4}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2}\Var(\gamma_0)  \\
			  & -  \frac{2 (1 - \bar{x})^2[\bar{x} (1-\bar{x})]}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2} \Cov(\gamma_0, \gamma_1)  \\
			  & + \frac{\bar{x}^2 (1-\bar{x})^2}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2} \Var(\gamma_1) \bigg) \\
\end{align*} 


\begin{align*}
	W_{women}^2 &= \left(\frac{(\bar{x})^4}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2}\right)\Var(\gamma_1)^2 \Var(\gamma_0)  \\
				& -  \frac{2 \bar{x}^2[\bar{x} (1-\bar{x})]}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2} \Var(\gamma_1) \Var(\gamma_0) \Cov(\gamma_0, \gamma_1)  \\
				& + \frac{\bar{x}^2 (1-\bar{x})^2}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2} \Var(\gamma_0)^2 \Var(\gamma_1) \\
				&= \Var(\gamma_0) \Var(\gamma_1) \bigg(\frac{(\bar{x})^4}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2}\Var(\gamma_1) \\
				& -  \frac{2 \bar{x}^2[\bar{x} (1-\bar{x})]}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2} \Cov(\gamma_0, \gamma_1)  \\
				& + \frac{\bar{x}^2 (1-\bar{x})^2}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2} \Var(\gamma_0) \bigg) \\
\end{align*} 


So letting $\Cov(\gamma_0, \gamma_1) = 0$ we get

\begin{align*}
	W_{men}^2 &= \Var(\gamma_0) \Var(\gamma_1) \bigg(\frac{(1 - \bar{x})^2 }{(1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1)} \bigg) \\
\end{align*} 

\begin{align*}
	W_{women}^2 &= \Var(\gamma_0) \Var(\gamma_1) \bigg(\frac{(\bar{x})^2 }{(1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1)} \bigg) \\
\end{align*} 



Now observe that if $\alpha = \Var(\gamma_1)$ and $\beta = \Var(\gamma_0)$ then
\begin{align*}
	\alpha W_{women}^2 + \beta W_{men}^2 &= \Var(\gamma_0) \Var(\gamma_1) \big[ \frac{\Var(\gamma_1) \bar{x}^2 + \Var(\gamma_0) (1- \bar{x})^2}{(1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1)} \big] \\
										 &= \Var(\gamma_0) \Var(\gamma_1)
\end{align*} 
So we can write
\begin{align*}
	\alpha W^{2}_{women} &= \Var(\gamma_0) \Var(\gamma_1) - \beta W^{2}_{men} \\
	W^{2}_{women} &= \frac{\Var(\gamma_0) \Var(\gamma_1) - \beta W^{2}_{men}}{\alpha} \\
	W^{2}_{women} &= \frac{\Var(\gamma_0) \Var(\gamma_1) - \Var(\gamma_0) W^{2}_{men}}{\Var(\gamma_1)} \\
\end{align*} 

And thus conclude

\begin{align*}
	MSE &= (1 - p) W^2_{women} + p W^2_{men} \\
	MSE &= p W^{2}_{men} + (1-p) \bigg(\frac{\Var(\gamma_0) \Var(\gamma_1) - \Var(\gamma_0) W^{2}_{men}}{\Var(\gamma_1)} \bigg) \\
	MSE &= p W^{2}_{men} + (1-p) \bigg(\frac{\Var(\gamma_0) \Var(\gamma_1) - \Var(\gamma_0) W^{2}_{men}}{\Var(\gamma_1)} \bigg) \\
	&= \frac{\bigg(p[\Var(\gamma_0) + \Var(\gamma_1)]- \Var(\gamma_0)\bigg) W^{2}_{men} + (1-p) \Var(\gamma_0) \Var(\gamma_1)}{\Var(\gamma_1)}
\end{align*} 

Thus when $p > \frac{\Var(\gamma_0)}{\Var(\gamma_0) + \Var(\gamma_1)}$ we can clearly see that MSE is minimized when $W_{men}^2$ is minimized (when $\bar{x} = 1$. And inversely when $p < \frac{\Var(\gamma_0)}{\Var(\gamma_0) + \Var(\gamma_1)}$ MSE is minimized when $W_{men}^2$ is maximaximized (when $\bar{x} = 0$.
In other words, when the proportion of men is  $p > \frac{\Var(\gamma_0)}{\Var(\gamma_0) + \Var(\gamma_1)}$ it is optimal to have only men ($\bar{x}$) in the trial, and vice versa. And when there are eqal number of men and women in the population,  $MSE$ does not depend on $W_{men}^2$ and thus has equal error of $\frac{\Var(\gamma_0)}{\Var(\gamma_0) + \Var(\gamma_1)}$ for any $\bar{x}$.

We can also solve this using the first order conditions.

\begin{align*}
	MSE = (1-p)W_{women}^2 + p W_{men}^2 &= \Var(\gamma_0) \Var(\gamma_1) \big[ \frac{(1-p)\bar{x}^2 + p(1- \bar{x})^2}{(1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1)} \big]
\end{align*} 

Which gives a derivative of
\begin{align*}
	\frac{MSE}{d\bar{x}} = -\frac{ 2 \Var(\gamma_0) \Var(\gamma_1) (1 - \bar{x}) \bar{x} \big[\Var(\gamma_0) (p - 1) + \Var(\gamma_1) p\big] }{(\Var(\gamma_0) (\bar{x} - 1)^2 + \Var(\gamma_1) \bar{x}^2)^2}
\end{align*} 

Here we can see that we still have roots $\bar{x} = 0$ and $\bar{x} = 1$ and all that changes is we get a variances-weighted edge-case whenever $(\Var(\gamma_0) + \Var(\gamma_1))p - \Var(\gamma_0) = 0$

And to get which is the minizing solution we can observe the SOC 
\begin{align*}
	\frac{MSE}{d \bar{x}^2} &= -\frac{2 \Var(\gamma_0) \Var(\gamma_1) [ \Var(\gamma_0) (p - 1) + \Var(\gamma_1) p ] [ \Var(\gamma_0) (2 x + 1) (x - 1)^2 + \Var(\gamma_1) x^2 (2 x - 3) ]}{[ \Var(\gamma_0) (x - 1)^2 + \Var(\gamma_1) x^2 ]^3}
\end{align*} 

From the second order condition, we can look at the two roots. We can see that the numerator reduces to two cases. When $p > \frac{\Var(\gamma_0)}{(\Var(\gamma_0) + \Var(\gamma_1))}$, then we can observe that the numerator is positive for the root $\bar{x} = 1$. Conversely, the numerator is positive when  $\bar{x} = 0$
We can also see this outlined in the simulations below.
% insert png figure named Figure_1.png here
\begin{figure}[ht!]
	\label{fig:simulate-sym}
  \centering
	\includegraphics[width=0.8\textwidth]{simulate-sym}
  % \caption{Plot of $\frac{\partial J}{\partial \bar{x}}$}
	\caption{First column contains the simulated error for different population proportions with the population proportion shown by the blue vertical line and point of minimum error shown with the red vertical line. The second column has the analytic error computed in the math above as well as a vertical line showing the first critical point. The third column shows the first derivative with respect to $\bar{x}$.} 
\end{figure}





\subsection*{2. Just Men}

\begin{align*}
	\beta_i &= \gamma_0 + x \gamma_1 \\
	\beta_{i}^{post} &= \E(\gamma_0) + \E(\gamma_1)  + c_i ( \gamma_0 + \bar{x} \gamma_1 - (\E(\gamma_0) + \bar{x} \E(\gamma_1))) \\
	\beta_i^{women} &= \gamma_0 \\
	\beta_i^{men} &= \gamma_0 + \gamma_1 \\
	\beta_i^{post,men} &= \E(\gamma_0) + \E(\gamma_1)  + c_{men} ( \gamma_0 + \bar{x} \gamma_1 - (\E(\gamma_0) + \bar{x} \E(\gamma_1))) \\
	\beta_i^{post, women} &= \E(\gamma_0) +  c_{women} ( \gamma_0 + \bar{x} \gamma_1 - (\E(\gamma_0) + \bar{x} \E(\gamma_1))) \\
\end{align*} 

And we have
\begin{align*}
	C_i &= \frac{\Var(\gamma_0) + x \bar{x} \Var(\gamma_1)}{\Var(\gamma_0) + \bar{x}^2 \Var{\gamma_1)}} \\
	C_{women} &= \frac{\Var(\gamma_0)}{\Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1)} \\
	C_{men} &= \frac{\Var(\gamma_0) + \bar{x} \Var(\gamma_1)}{\Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1)}
\end{align*} 

\begin{align*}
	W_{women} &=  \beta_i^{women} - \beta_i^{post,women} \\
			&= (\gamma_0 - \E(\gamma_0)) - C_{men} \big[(\gamma_0 - \E(\gamma_0)) + \bar{x} ( \gamma_1 - \E(\gamma_1)) \big] \\
			&= ( 1 - c_{women}) (\gamma_0 - \E(\gamma_0)) - C_{women} \bar{x} (\gamma_1 - \E(\gamma_1)) \\
	W_{men} &=  \beta_i^{men} - \beta_i^{post,men} \\
			&= (\gamma_0 - \E(\gamma_0)) + (\gamma_1 - \E(\gamma_1) - C_{men} \big[(\gamma_0 - \E(\gamma_0)) + \bar{x} ( \gamma_1 - \E(\gamma_1)) \big] \\
			&= ( 1 - c_{men}) (\gamma_0 - \E(\gamma_0)) + (1 - C_{men} \bar{x}) (\gamma_1 - \E(\gamma_1))
\end{align*} 

Squaring
\begin{align*}
	W_{women}^2 &= (1 - c_{wom})^2 \Var(\gamma_0) - 2( 1 - c_{wom})c_{wom} \bar{x}\Cov(\gamma_0, \gamma_1) + c_{wom}^2 \bar{x} \Var(\gamma_1) \\
	W_{men}^2 &= (1 - c_{men})^2 \Var(\gamma_0) + 2( 1 - c_{men})(1 - c_{men} \bar{x})\Cov(\gamma_0, \gamma_1) + (1 - c_{men} \bar{x})^2 \Var(\gamma_1) \\
\end{align*} 

And assuming $\Cov(\gamma_0, \gamma_1) = 0$ we get

\begin{align*}
	W_{women}^2 &= (1 - c_{wom})^2 \Var(\gamma_0) + c_{wom}^2 \bar{x}^2 \Var(\gamma_1) \\
				&= \frac{\bar{x}^4 \Var(\gamma_1)^2}{(\Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2} \Var(\gamma_0) + \frac{\bar{x}^2 \Var(\gamma_0)^2}{(\Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2} \Var(\gamma_1) \\
				&= \bar{x}^2 \Var(\gamma_1) \Var(\gamma_0)\frac{\bar{x}^2 \Var(\gamma_1) + \Var(\gamma_0)}{(\Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2} \\
				&= \frac{\bar{x}^2 \Var(\gamma_1) \Var(\gamma_0)}{\Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1)} \\
	W_{men}^2 &= (1 - c_{men})^2 \Var(\gamma_0) + (1 - c_{men} \bar{x})^2 \Var(\gamma_1) \\
	          &= \frac{\Var(\gamma_1)^2[\bar{x}^2 - \bar{x}]^2}{(\Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2} \Var(\gamma_0) + \frac{\Var(\gamma_0)^2[1 - \bar{x}]^2}{(\Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2} \Var(\gamma_1) \\
			  &= \frac{\Var(\gamma_1)^2 \bar{x}^2 [\bar{x} - 1]^2}{(\Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2} \Var(\gamma_0) + \frac{\Var(\gamma_0)^2[\bar{x} - 1]^2}{(\Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2} \Var(\gamma_1) \\
			  &= \Var(\gamma_0) \Var(\gamma_1) [\bar{x} - 1]^2 \frac{\Var(\gamma_1)\bar{x}^2 + \Var(\gamma_0)}{(\Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2}\\
			  &= \frac{\Var(\gamma_0) \Var(\gamma_1) [\bar{x} - 1]^2 }{\Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1)}\\
\end{align*} 

So we get MSE of
\begin{align*}
	MSE &= \Var(\gamma_0) \Var(\gamma_1) \frac{(1-p) \bar{x}^2 + p[\bar{x} - 1]^2 }{\Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1)}\\
\end{align*} 

Taking first order conditions we get

\begin{align*}
	\frac{d}{dx} &\frac{\Var(\gamma_0) \Var(\gamma_1) [ (1 - p) x^2 + p (x - 1)^2 ]}{\Var(\gamma_0) + \Var(\gamma_1) x^2} \\
&= 2 \Var(\gamma_0) \Var(\gamma_1) \frac{ \Var(\gamma_0) (x - p) + \Var(\gamma_1) p (x - 1) x }{(\Var(\gamma_0) + \Var(\gamma_1) x^2)^2} = 0 \\
&\iff \Var(\gamma_0) (x - p) + \Var(\gamma_1) p (x - 1) x = 0 \\
&\iff x^2 \Var(\gamma_1) p + x(\Var(\gamma_0) - \Var(\gamma_1) p) - \Var(\gamma_0) p = 0
\end{align*} 

For this we can use the quadratic formula with
\begin{enumerate}
	\item $a = \Var(\gamma_1) p$
	\item $b = \Var(\gamma_0) - \Var(\gamma_1) p$
	\item $c = - \Var(\gamma_0) p$
\end{enumerate}

giving us

\begin{align*}
	x &= \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} \\
	  &= \frac{-(\Var(\gamma_0) - \Var(\gamma_1) p) \pm \sqrt{(\Var(\gamma_0) - \Var(\gamma_1) p)^2 + 4 \Var(\gamma_0) \Var(\gamma_1) p^2 }}{2 \Var(\gamma_1) p} \\
	  &= \frac{\Var(\gamma_1) p-\Var(\gamma_0)  \pm \sqrt{\Var(\gamma_0)^2 - 2\Var(\gamma_0) \Var(\gamma_1) p + \Var(\gamma_1)^2 p^2 + 4 \Var(\gamma_0) \Var(\gamma_1) p^2 }}{2 \Var(\gamma_1) p} \\
	  &= \frac{\Var(\gamma_1) p-\Var(\gamma_0)  \pm \sqrt{\Var(\gamma_0)^2 + 2\Var(\gamma_0) \Var(\gamma_1) p(2p -1) + \Var(\gamma_1)^2 p^2}}{2 \Var(\gamma_1) p} \\
\end{align*} 

We can see the errors and optimal solutions \ref{fig:v2-intercept} and \ref{fig:v2-intercept-optim}


\begin{figure}[ht!]
\label{fig:v2-intercept}
  \centering
	\includegraphics[width=0.9\textwidth]{v2-intercept}
  % \caption{Plot of $\frac{\partial J}{\partial \bar{x}}$}
	\caption{First column contains the simulated error for different population proportions with the population proportion shown by the blue vertical line and point of minimum error shown with the red vertical line. The second column has the analytic error computed in the math above as well as a vertical line showing the first critical point and blue line showing population proportion. The third column shows the first derivative with respect to $\bar{x}$ and where it equals zero.} 
\end{figure}

\begin{figure}[ht!]
\label{fig:v2-intercept-optim}
  \centering
	\includegraphics[width=0.8\textwidth]{Intercept_optimal}
  % \caption{Plot of $\frac{\partial J}{\partial \bar{x}}$}
	\caption{Here we show the optimal trial proportion of men vs the population proprtion (on the y and x axis respectively) for equal variances of 1.} 
\end{figure}

\subsection*{3. Generalization of case 1 and 2}

In case 1, we solved 

\begin{align*}
	\beta_i = (1 - x) \gamma_0 + x \gamma_1 = \gamma_0 + x(\gamma_0 + \gamma_1)
\end{align*} 

Thus in effect, the constant and coefficient are correlated through $\gamma_0$. We can thus nest both cases by allowing for arbitrary correlation in case 1.


We will do this below

First for $C$ 

We have that
\begin{align*}
	c = \frac{\Cov (x \gamma, \bar{x} \gamma)}{ \Var( \bar{x} \gamma )}
\end{align*} 

so for individual i, $c$ reduces to

\begin{align*}
		c &= \frac{\Var(\gamma_0) + (x + \bar{x}) \Cov(\gamma_0, \gamma_1) + x \bar{x} \Var(\gamma_1)}{\Var(\gamma_0) + 2 \bar{x} \Cov(\gamma_0, \gamma_1) + \bar{x}^2\Var(\gamma_1)} \\
		C_{women} &= \frac{\Var(\gamma_0) + \bar{x} \Cov(\gamma_0, \gamma_1)}{\Var(\gamma_0) + 2 \bar{x} \Cov(\gamma_0, \gamma_1) + \bar{x}^2\Var(\gamma_1)} \\
	C_{men} &= \frac{\Var(\gamma_0) + (1 + \bar{x}) \Cov(\gamma_0, \gamma_1) + \bar{x} \Var(\gamma_1)}{\Var(\gamma_0) + 2 \bar{x} \Cov(\gamma_0, \gamma_1) + \bar{x}^2\Var(\gamma_1)} \\
\end{align*} 

We still have

\begin{align*}
	\beta_i &= \gamma_0 + x \gamma_1 \\
	\beta_{i}^{post} &= \E(\gamma_0) + \E(\gamma_1)  + c_i ( \gamma_0 + \bar{x} \gamma_1 - (\E(\gamma_0) + \bar{x} \E(\gamma_1))) \\
	\beta_i^{women} &= \gamma_0 \\
	\beta_i^{men} &= \gamma_0 + \gamma_1 \\
	\beta_i^{post,men} &= \E(\gamma_0) + \E(\gamma_1)  + c_{men} ( \gamma_0 + \bar{x} \gamma_1 - (\E(\gamma_0) + \bar{x} \E(\gamma_1))) \\
	\beta_i^{post, women} &= \E(\gamma_0) +  c_{women} ( \gamma_0 + \bar{x} \gamma_1 - (\E(\gamma_0) + \bar{x} \E(\gamma_1))) \\
\end{align*} 

and

\begin{align*}
	W_{women} &=  \beta_i^{women} - \beta_i^{post,women} \\
			&= (\gamma_0 - \E(\gamma_0)) - C_{men} \big[(\gamma_0 - \E(\gamma_0)) + \bar{x} ( \gamma_1 - \E(\gamma_1)) \big] \\
			&= ( 1 - c_{women}) (\gamma_0 - \E(\gamma_0)) - C_{women} \bar{x} (\gamma_1 - \E(\gamma_1)) \\
	W_{men} &=  \beta_i^{men} - \beta_i^{post,men} \\
			&= (\gamma_0 - \E(\gamma_0)) + (\gamma_1 - \E(\gamma_1) - C_{men} \big[(\gamma_0 - \E(\gamma_0)) + \bar{x} ( \gamma_1 - \E(\gamma_1)) \big] \\
			&= ( 1 - c_{men}) (\gamma_0 - \E(\gamma_0)) + (1 - C_{men} \bar{x}) (\gamma_1 - \E(\gamma_1))
\end{align*} 

Squaring
\begin{align*}
	W_{women}^2 &= (1 - c_{wom})^2 \Var(\gamma_0) - 2( 1 - c_{wom})c_{wom} \bar{x}\Cov(\gamma_0, \gamma_1) + c_{wom}^2 \bar{x}^2 \Var(\gamma_1) \\
				&=\frac{\big(\bar{x} \Cov(\gamma_0, \gamma_1) + \bar{x}^2 \Var(\gamma_1)\big)^2}{\big( \Var(\gamma_0) + 2 \bar{x} \Cov(\gamma_0, \gamma_1) + \bar{x}^2\Var(\gamma_1)\big)^2} \Var(\gamma_0) -  \\
				&2 \bigg(\frac{[ \bar{x} \Cov(\gamma_0, \gamma_1) + \bar{x}^2 \Var(\gamma_1) ][ \Var(\gamma_0) + \bar{x} \Cov(\gamma_0, \gamma_1) ]}{\big( \Var(\gamma_0) + 2 \bar{x} \Cov(\gamma_0, \gamma_1) + \bar{x}^2\Var(\gamma_1)\big)^2} \bigg) \bar{x}\Cov(\gamma_0, \gamma_1) + \\
				&\frac{\big(\Var(\gamma_0) + \bar{x} \Cov(\gamma_0, \gamma_1) \big)^2}{\big( \Var(\gamma_0) + 2 \bar{x} \Cov(\gamma_0, \gamma_1) + \bar{x}^2\Var(\gamma_1)\big)^2} \bar{x}^2 \Var(\gamma_1) \\ 
				&=\frac{\bar{x}^2 \Cov(\gamma_0, \gamma_1)^2 + 2\bar{x}^3 \Cov(\gamma_0, \gamma_1) \Var(\gamma_1) + \bar{x}^4 \Var(\gamma_1)^2 }{\big( \Var(\gamma_0) + 2 \bar{x} \Cov(\gamma_0, \gamma_1) + \bar{x}^2\Var(\gamma_1)\big)^2} \Var(\gamma_0)  \\
				&- 2 \bigg(\frac{\bar{x} \Cov(\gamma_0, \gamma_1)\Var(\gamma_0) + \bar{x}^2 \Cov(\gamma_0, \gamma_1)^2 + \bar{x}^2 \Var(\gamma_0) \Var(\gamma_1)  + \bar{x}^3 \Var(\gamma_1)\Cov(\gamma_0, \gamma_1)}{\big( \Var(\gamma_0) + 2 \bar{x} \Cov(\gamma_0, \gamma_1) + \bar{x}^2\Var(\gamma_1)\big)^2} \bigg) \bar{x}\Cov(\gamma_0, \gamma_1) \\
				&+ \frac{\Var(\gamma_0)^2 + 2 \bar{x} \Var(\gamma_0) \Cov(\gamma_0, \gamma_1) + \bar{x}^2 \Cov(\gamma_0, \gamma_1)^2 }{\big( \Var(\gamma_0) + 2 \bar{x} \Cov(\gamma_0, \gamma_1) + \bar{x}^2\Var(\gamma_1)\big)^2} \bar{x}^2 \Var(\gamma_1) \\ 
				  &= \frac{\bar{x}^2 \big[ \Var(\gamma_0) \Var(\gamma_1) - \Cov(\gamma_0, \gamma_1)^2 \big]}{\Var(\gamma_0) + \Var(\gamma_1) \bar{x}^2 + 2 \Cov(\gamma_0, \gamma_1) \bar{x}}
\end{align*} 

\begin{align*}
	W_{men}^2 &= (1 - c_{men})^2 \Var(\gamma_0) + 2( 1 - c_{men})(1 - c_{men} \bar{x})\Cov(\gamma_0, \gamma_1) + (1 - c_{men} \bar{x})^2 \Var(\gamma_1) \\
			  &= \frac{\big((\bar{x} - 1) \Cov(\gamma_0, \gamma_1) + (\bar{x}^2 - \bar{x}) \Var(\gamma_1)\big)^2}{\big(\Var(\gamma_0) + 2 \bar{x} \Cov(\gamma_0, \gamma_1) + \bar{x}^2\Var(\gamma_1)\big)^2} \Var(\gamma_0) \\
			  &+ 2\frac{[(\bar{x} - 1) \Cov(\gamma_0, \gamma_1) + (\bar{x}^2 - \bar{x}) \Var(\gamma_1)][(1 - \bar{x}) \Var(\gamma_0) + (\bar{x} - \bar{x}^2) \Cov(\gamma_0, \gamma_1) ]}{\big(\Var(\gamma_0) + 2 \bar{x} \Cov(\gamma_0, \gamma_1) + \bar{x}^2\Var(\gamma_1) \big)^2}\Cov(\gamma_0, \gamma_1) \\
			  &+ \frac{[(1 - \bar{x}) \Var(\gamma_0) + (\bar{x} - \bar{x}^2) \Cov(\gamma_0, \gamma_1)]^2}{\big(\Var(\gamma_0) + 2 \bar{x} \Cov(\gamma_0, \gamma_1) + \bar{x}^2\Var(\gamma_1) \big)^2} \Var(\gamma_1) \\
			  &= \frac{(\bar{x} - 1)^2 \big[ \Var(\gamma_0) \Var(\gamma_1) - \Cov(\gamma_0, \gamma_1)^2 \big]}{\Var(\gamma_0) + \Var(\gamma_1) \bar{x}^2 + 2 \Cov(\gamma_0, \gamma_1) \bar{x}}
\end{align*} 

We can see then that if $\Cov(\gamma_0, \gamma_1) = 0$ we recover our result from version 2.


And we get a mean square error of 
\begin{align*}
	MSE = \big(\Var(\gamma_0) \Var(\gamma_1) - \Cov(\gamma_0, \gamma_1)^2 \big)\frac{(1 - p) \bar{x}^2  + p(\bar{x} - 1)^2 }{\Var(\gamma_0) + \Var(\gamma_1) \bar{x}^2 + 2 \Cov(\gamma_0, \gamma_1) \bar{x}}
\end{align*} 

\begin{align*}
	\frac{d}{dx} &= \frac{((\Var(\gamma_0) \Var(\gamma_1) - \Cov(\gamma_0, \gamma_1)^2) ((1 - p) \bar{x}^2 + p (\bar{x} - 1)^2))}{ \Var(\gamma_0) + \Var(\gamma_1) \bar{x}^2 + 2 \bar{x} \Cov(\gamma_0, \gamma_1) } \\
	&= \frac{2 (\Var(\gamma_0) \Var(\gamma_1) - \Cov(\gamma_0, \gamma_1)^2) (\Var(\gamma_0) (\bar{x} - p) + \Var(\gamma_1) p (\bar{x} - 1) \bar{x} + \Cov(\gamma_0, \gamma_1) (\bar{x}^2 - p)) }{(\Var(\gamma_0) + \bar{x} (\Var(\gamma_1) \bar{x} + 2 \Cov(\gamma_0, \gamma_1)))^2}
\end{align*} 

Thus we can use the quadratic formula to determine the optimal trial proportions.

\begin{align*}
	\bar{x} = \frac{\Var(\gamma_1) p - \Var(\gamma_0) - \pm \sqrt{ 4 p \big[ \Var(\gamma_0) + \Cov(\gamma_0, \gamma_1) \big] \big[ \Var(\gamma_1) p + \Cov(\gamma_0, \gamma_1) \big] + \big(\Var(\gamma_0) - \Var(\gamma_1) p\big)^2 } }{ 2 (\Var(\gamma_1) p + \Cov(\gamma_0, \gamma_1)) } 
\end{align*} 
and clearly $\Var(\gamma_1) p + \Cov(\gamma_0, \gamma_1) \neq 0$

\end{document}
