% {{{
\input{$HOME/.config/nvim/snippets/.math.preamble.tex}
% }}}

% Title {{{
\begin{document}

\begin{center}
	{\large \bf Jason Abaluck }   \\ \large optimal Representation \\ Ephraim Sutherland
\end{center}
% }}}

\tableofcontents


% First we'll start by examing a simple scenario where two subgroups have equal variance (e.g. considermen and women).


\subsection*{Setup}


\begin{enumerate}

	\item  Suppose a physician can only see see ATE and some measure of representativeness. They have prior
		$\bar{\beta}$ and  $\beta_{ATE} = (1/N) \sum \beta_{i}$.

	\item need model for betas related to each other based on $x$'s.
		WLOG, suppose
		\begin{align*}
			\beta(x_i) = x_i \gamma
		\end{align*} 
		Where $x_i$ is a vector of characteristics and $\gamma$ is a vector of coefficients. \\
		If you know $\gamma$, then you know $\beta$ for any given patient.
	\item However, you don't observe $\gamma$, you instead observe:
		$\beta_{ATE} = \bar x \gamma$ where $\bar x = (\frac{1}{N}) \sum x_i$
	\item We know $\beta_i$ for patients with characteristics $\bar x$ (it is $\beta_{ATE}$).
	\item For other patients, need to solve
		\begin{align*}
			\beta_{i,post} = \E(x_i \gamma | \bar x \gamma = \beta_{ATE})
		\end{align*} 

	\item to solve
		\begin{enumerate}
			\item
				\begin{align*}
					\beta_{i,post} & = \E (x_i \gamma | \bar x \gamma = \beta_{ATE})                                                                        \\
								   & = \E((x_i - c_i \bar{x}) \gamma | \bar{x} \gamma = \beta_{ATE}) + c_i \E(\bar{x} \gamma | \bar{x} \gamma = \beta_{ATE}) \\
								   & = \E((x_{i} - c_i \bar{x}) \gamma | \bar{x} \gamma = \beta_{ATE}) + c_i \beta_{ATE}                                       \\
				\end{align*}
				For any constant $c_i$. \\
				Choose $c_i$ so that 
				\begin{align*}
					\Cov((x_{i} - c_i \bar{x})\gamma, \bar{x} \gamma) = 0
				\end{align*} 
				maybe assume normality so that this guarantees independence.
				Then,
				\begin{align*}
					\E((x_i - c_i \bar{x}) \gamma | \bar{x} \gamma = \beta_{ATE}) = (x_{i} - c_i \bar{x}) \E(\gamma)
				\end{align*}
				So then
				\begin{align*}
					(x_i - c_i \bar{x}) \E(\gamma) + c_i \beta_{ATE} = x_i \E(\gamma) + c_i (\beta_{ATE} - \bar{x} \E(\gamma))
				\end{align*} 
				($c_i$ depends on $x_i$)

				In other words, your belief is your prior, adjusted based on the difference between the observed ATE and your prior about the ATE.
				The key question is how much adjustment you do which depends on $"c_i"$. We choose $c_i$ to solve:

				\begin{align*}
                          & \Cov ((x_i - c_i \bar{x}) \gamma, \bar{x} \gamma) = 0                          \\
				 \iff     & \Cov(x_i \gamma, \bar{x} \gamma) -c_i \Cov(\bar{x} \gamma, \bar{x} \gamma) = 0 \\
					\iff  & \Cov(x_i \gamma, \bar{x} \gamma) = c_i \Var(\bar{x} \gamma)                  \\
				\iff & c_i = \frac{\Cov(\beta_i, \beta_{ATE}) }{ \Var(\beta_{ATE})}
				\end{align*} 
				The random variable in this context is $\gamma$ (the coefficients on the $x $'s) in this case $\Var(\beta_{ATE})$ is a measure of how uncertain one was about what $\beta_{ATE}$ would be before doing the trial.

				$c_i$ is the equation for a regression of $\beta_i$ on $\beta_{ATE}$. In other words, we take a bunch of patients with characteristics $x_i$ and we keep redrawing the gammas from our prior distribution the we ask how correlated  $\beta_i$ and $\beta_{ATE}$ are. If they are more correlated (as they would be for patients where the $x_i$ are closer to $\bar{x}$ we update more.

				To compute $c_i$, we just need to know $x_i$ $\bar{x}$, and the distribution of $\gamma$.

				Suppose we want to design the trial to minimize:
				\begin{align*}
					\min \E[(\beta_i - \beta_{i,post})^2]
				\end{align*} 
		\end{enumerate}

\end{enumerate} 

\subsection*{Simple Cases}
\begin{enumerate}
	\item There is just one $x$ and it is binary (old v young). Can it be solved analytically?
	\item Can you solve a 2-dimensional case? 
\end{enumerate}


First observe that in our current setup, $c$ does not depend on the $\gamma$'s

We have that
\begin{align*}
	c = \frac{\Cov (x \gamma, \bar{x} \gamma)}{ \Var( \bar{x} \gamma )}
\end{align*} 

so for individual i, $c$ reduces to


\begin{align*}
	c &= \frac{1^2\Var(\gamma_0) + x \bar{x} \Var(\gamma_1)}{1^2\Var(\gamma_0) + \bar{x}^2\Var(\gamma_1)} \quad \text{because $\Var(\gamma_0) = \Var(\gamma_1) = \Var(\gamma) = 1$} \\
	&= \frac{(1 + x \bar{x})\Var(\gamma)}{(1 + \bar{x}^2)\Var(\gamma)} = \frac{x' \bar{x}}{\bar{x}' \bar{x}}
\end{align*} 




\section{Appendix}

\subsection*{Derivations}

Recall we want to minimize
\begin{align*}
\min \E_x \left(\E_{\gamma_{0,1}}[(\beta_i - \beta_{i,post})^2]\right)
\end{align*}

One way we can rewrite these equations is as the effect of women vs men.
let the squared error (SE) be
\begin{align*}
	\sqrt{SE} &= \beta_i -  \beta_{i,post} = \underbrace{[(1-x) \gamma_0 + x \gamma_1]}_{\text{$\beta_i$}}  - \underbrace{[((x - c \bar{x})\E(\gamma) + c \beta_{ATE})]}_\text{$\beta_{i,post}$} \\
	  &= [(1-x) \gamma_0 + x \gamma_1]  - [((1-x) - c (1 - \bar{x})) \E(\gamma_0) + (x - c \bar{x})\E(\gamma_1) + c((1-\bar{x}) \gamma_0 + \bar{x} \gamma_1)] \\
	  % &= [(1-x) \gamma_0 + x \gamma_1  - (x - c \bar{x})\E(\gamma) + c((1-\bar{x}) \gamma_0 + \bar{x} \gamma_1)] \\
\end{align*}

$\beta_i$ $\beta_{i, post}$ and as a result, $SE$ are all a function of $x$. We can then describe

$\beta_{i}^{men} = \beta_i (x=1)$ and similarly for other terms to get

So 
\begin{align*}
	\beta_i^{men} &= \gamma_1 \\
	\beta_i^{post,men} &= -c_{men} ( 1- \bar{x}) \bar{\gamma_0} + (1 - c_{men}\bar{x}) \bar{\gamma_1} + c_{men} [ (1-\bar{x}) \gamma_0 + \bar{x}\gamma_1] \\
	\beta_i^{women} &= \gamma_0 \\
	\beta_i^{post,women} &= [ 1 - c_{wom}(1-\bar{x}) ]\E(\gamma_0) - c_{wom} \bar{x} \E(\gamma_1) + c_{wom} (1- \bar{x}) \gamma_0 + c_{wom} \bar{x} \gamma_1 \\
\end{align*} 

this can be broken down into

\begin{align*}
	\sqrt{ SE } &= (\beta_i^{men} - \beta_i^{post, men} )+ (\beta_i^{wom} - \beta_i^{post, wom}) \\
\end{align*} 

let 
 \begin{align*}
	 W^{men} &=  (\beta_i^{men} - \beta_i^{post, men}) \\
			&= (1 - \bar{x} c_{men}) (\gamma_1 - \bar{\gamma_1}) - c_{men} ( 1 - \bar{x}) (\gamma_0 - \bar{\gamma_0}) \\
			&= (c_{wom} (1 - \bar{x})) (\gamma_1 - \bar{\gamma_1}) - c_{men} ( 1 - \bar{x}) (\gamma_0 - \bar{\gamma_0}) \\
	 W^{wom} &=  (\beta_i^{wom} - \beta_i^{post, wom}) \\
			 &= [ 1 - c_{wom}(1 - \bar{x}) ] (\gamma_0 - \bar{\gamma_0}) - c_{wom} \bar{x} (\gamma_1 - \bar{\gamma_1}) \\
			 &= [ c_{men} \bar{x}] (\gamma_0 - \bar{\gamma_0}) - c_{wom} \bar{x} (\gamma_1 - \bar{\gamma_1}) \\
\end{align*} 
and recall that
\begin{align*}
	c_{i} &= \frac{(1-x)(1-\bar{x})\Var(\gamma_0) + x\bar{x}\Var(\gamma_1)}{(1-\bar{x})^2\Var(\gamma_0) + \bar{x}^2\Var(\gamma_1)} \\
		  &= \frac{(1-x)(1-\bar{x}) + x\bar{x}}{(1-\bar{x})^2 + \bar{x}^2} 
\end{align*}

so if you are a man, then 

\begin{align*}
	c_{men} =  \frac{\bar{x}\Var(\gamma_1)}{(1-\bar{x})^2\Var(\gamma_0) + \bar{x}^2\Var(\gamma_1)}
\end{align*}

and likewise if you are a woman, then

\begin{align*}
	c_{woman} = \frac{(1-\bar{x})\Var(\gamma_0)}{(1-\bar{x})^2\Var(\gamma_0) + \bar{x}^2\Var(\gamma_1)}
\end{align*}


and 
\begin{align*}
	W_{men}^2 &= \left(\frac{(1-\bar{x})^4}{((1-\bar{x})^2 + \bar{x}^2)^2}\right) \Var(\gamma_1) +  \frac{\bar{x}^2 (1-\bar{x})^2}{((1-\bar{x})^2 + \bar{x}^2)^2} \Var(\gamma_0) \\
			  &= \big(\frac{(1-\bar{x})^2 \Var(\gamma_1)} {(1-\bar{x})^2 + \bar{x}^2} \\
	W_{women}^2 &= \left(\frac{(\bar{x})^4}{((1-\bar{x})^2 + \bar{x}^2)^2}\right) \Var(\gamma_0) + \frac{\bar{x}^2 (1-\bar{x})^2}{((1-\bar{x})^2 + \bar{x}^2)^2} \Var(\gamma_1) \\
				&= \frac{(\bar{x})^2 \Var(\gamma_0) }{(1-\bar{x})^2 + \bar{x}^2} \\
\end{align*} 


so then taking the mean ($E_{x}$) with respect to $x$, we can write the mean squared error (MSE) as
\begin{align*}
	MSE &= p W_{men}^2 + (1 - p)W_{wom}^2 \\
	&= p \left(\frac{(1-\bar{x})^4\Var(\gamma_1) + ( \bar{x}^2 (1-\bar{x})^2 ) \Var(\gamma_0)}{((1-\bar{x})^2 + \bar{x}^2)^2}\right) + (1-p)\left(\frac{(\bar{x})^4 \Var(\gamma_0) + ( \bar{x}^2 (1-\bar{x})^2 ) \Var(\gamma_1) }{((1-\bar{x})^2 + \bar{x}^2)^2}\right) \\
\end{align*} 

so using the fact that $\Var(\gamma_0) = \Var(\gamma_1) = 1$ we get

\begin{align*}
	 MSE &= p \left(\frac{(1-\bar{x})^4 + ( \bar{x}^2 (1-\bar{x})^2 ) }{((1-\bar{x})^2 + \bar{x}^2)^2}\right) + (1-p)\left(\frac{(\bar{x})^4  + ( \bar{x}^2 (1-\bar{x})^2 ) }{((1-\bar{x})^2 + \bar{x}^2)^2}\right) \\
	 MSE &= p \left(\frac{(1-\bar{x})^2 }{(1-\bar{x})^2 + \bar{x}^2}\right) + (1-p)\left(\frac{(\bar{x})^2 }{(1-\bar{x})^2 + \bar{x}^2}\right) \\
			 &= p \frac{(1-\bar{x})^2}{(1-\bar{x})^2 + \bar{x}^2} + (1-p) \frac{\bar{x}^2}{(1-\bar{x})^2 + \bar{x}^2}
\end{align*} 

In other words for individual $i$ we have that

\begin{align*}
	 MSE &= p (1 - c_{men} \bar{x}) + (1-p) (1 - c_{women} (1 - \bar{x})) \\
	 MSE &= p \left(1 - \frac{\bar{x}^2}{(1-\bar{x})^2 + \bar{x}^2}\right) + (1-p) \left(1 - \frac{(1-\bar{x})^2}{(1-\bar{x})^2 + \bar{x}^2} \right) \\
	 MSE &= p \left(\frac{(1 - \bar{x})^2}{(1-\bar{x})^2 + \bar{x}^2}\right) + (1-p) \left(\frac{\bar{x}^2}{(1-\bar{x})^2 + \bar{x}^2} \right) \\
	 MSE &= p \left[ (1 - \bar{x})c_{wom} \right] + (1-p) \left[ \bar{x}c_{men} \right] \\
\end{align*} 

Interestingly this implies a FOC of

\begin{align*}
	\frac{\partial MSE }{\partial \bar{x}} &= p \left(\frac{2 \bar{x} (1 - \bar{x})}{((1-\bar{x})^2 + \bar{x}^2)^2}\right) - (1-p) \left(\frac{2 \bar{x} (1 - \bar{x})}{((1-\bar{x})^2 + \bar{x}^2)^2} \right) = 0 \\
	\frac{\partial MSE}{\partial \bar{x}} &= (2p -1) \left(\frac{2 \bar{x} (1 - \bar{x})}{((1-\bar{x})^2 + \bar{x}^2)^2}\right) = 0
\end{align*} 

meaning we've retrieved our original FOC.

In full generality we also can say

\begin{align*}
	W_{men}^2 &= \left(\frac{(1 - \bar{x})^4}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2}\right)\Var(\gamma_0)^2 \Var(\gamma_1)  \\
			  & -  \frac{2 (1 - \bar{x})^2[\bar{x} (1-\bar{x})]}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2} \Var(\gamma_1) \Var(\gamma_0) \Cov(\gamma_0, \gamma_1)  \\
			  & + \frac{\bar{x}^2 (1-\bar{x})^2}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2} \Var(\gamma_1)^2 \Var(\gamma_0) \\
			  &= \Var(\gamma_0) \Var(\gamma_1) \bigg(\frac{(1 - \bar{x})^4}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2}\Var(\gamma_0)  \\
			  & -  \frac{2 (1 - \bar{x})^2[\bar{x} (1-\bar{x})]}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2} \Cov(\gamma_0, \gamma_1)  \\
			  & + \frac{\bar{x}^2 (1-\bar{x})^2}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2} \Var(\gamma_1) \bigg) \\
\end{align*} 


\begin{align*}
	W_{women}^2 &= \left(\frac{(\bar{x})^4}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2}\right)\Var(\gamma_1)^2 \Var(\gamma_0)  \\
				& -  \frac{2 \bar{x}^2[\bar{x} (1-\bar{x})]}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2} \Var(\gamma_1) \Var(\gamma_0) \Cov(\gamma_0, \gamma_1)  \\
				& + \frac{\bar{x}^2 (1-\bar{x})^2}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2} \Var(\gamma_0)^2 \Var(\gamma_1) \\
				&= \Var(\gamma_0) \Var(\gamma_1) \bigg(\frac{(\bar{x})^4}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2}\Var(\gamma_1) \\
				& -  \frac{2 \bar{x}^2[\bar{x} (1-\bar{x})]}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2} \Cov(\gamma_0, \gamma_1)  \\
				& + \frac{\bar{x}^2 (1-\bar{x})^2}{((1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1))^2} \Var(\gamma_0) \bigg) \\
\end{align*} 


Now observe that
\begin{align*}
	W_{women}^2 + W_{women}^2 &= \Var(\gamma_0) \Var(\gamma_1) \big[ \frac{\bar{x}^2 + (1- \bar{x})^2}{(1-\bar{x})^2 \Var(\gamma_0) + \bar{x}^2 \Var(\gamma_1)} \big]
\end{align*} 

So if $\Var(\gamma_1) = \Var(\gamma_0) = 1$ then we get

\begin{align*}
	W_{women}^2 + W_{women}^2 = 1	
\end{align*} 

And so 
\begin{align*}
	MSE &= p W^2_{men} + (1 - p) W^2_{women}
\end{align*} 

but
\begin{align*}
	W^{2}_{men} = 1 - W^{2}_{women}
\end{align*} 

So we can conclude

\begin{align*}
	MSE &= p W^{2}_{men} + (1-p) (1 - W^{2}_{men}) \\
	&= (2p-1) W^{2}_{men} + (1-p) 
\end{align*} 

Thus when $p > \frac{1}{2}$ we can clearly see that MSE is minimized when $W_{men}^2$ is minimized (when $\bar{x} = 1$. And inversely when $p < \frac{1}{2}$ MSE is minimized when $W_{men}^2$ is maximaximized (when $\bar{x} = 0$. In other words, when the proportion of men is  $p > \frac{1}{2}$ it is optimal to have only men ($\bar{x}$) in the trial, and vice versa. And when there are eqal number of men and women in the population,  $MSE$ does not depend on $W_{men}^2$ and thus has equal error of $ \frac{1}{2}$ for any $\bar{x}$.

\end{document}

