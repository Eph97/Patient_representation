% {{{
\input{$HOME/.config/nvim/snippets/.math.preamble.tex}
% }}}

% Title {{{
\begin{document}

\begin{center}
	{\large \bf Jason Abaluck }   \\ \large optimal Representation \\ Ephraim Sutherland
\end{center}
% }}}

\tableofcontents


% First we'll start by examing a simple scenario where two subgroups have equal variance (e.g. considermen and women).


\subsection*{Setup}


\begin{enumerate}

	\item  Suppose a physician can only see see ATE and some measure of representativeness. They have prior
		$\bar{\beta}$ and  $\beta_{ATE} = (1/N) \sum \beta_{i}$.

	\item need model for betas related to each other based on $x$'s.
		WLOG, suppose
		\begin{align*}
			\beta(x_i) = x_i \gamma
		\end{align*} 
		Where $x_i$ is a vector of characteristics and $\gamma$ is a vector of coefficients. \\
		If you know $\gamma$, then you know $\beta$ for any given patient.
	\item However, you don't observe $\gamma$, you instead observe:
		$\beta_{ATE} = \bar x \gamma$ where $\bar x = (\frac{1}{N}) \sum x_i$
	\item We know $\beta_i$ for patients with characteristics $\bar x$ (it is $\beta_{ATE}$).
	\item For other patients, need to solve
		\begin{align*}
			\beta_{i,post} = \E(x_i \gamma | \bar x \gamma = \beta_{ATE})
		\end{align*} 

	\item to solve
		\begin{enumerate}
			\item
				\begin{align*}
					\beta_{i,post} & = \E (x_i \gamma | \bar x \gamma = \beta_{ATE})                                                                        \\
								   & = \E((x_i - c_i \bar{x}) \gamma | \bar{x} \gamma = \beta_{ATE}) + c_i \E(\bar{x} \gamma | \bar{x} \gamma = \beta_{ATE}) \\
								   & = \E((x_{i} - c_i \bar{x}) \gamma | \bar{x} \gamma = \beta_{ATE}) + c_i \beta_{ATE}                                       \\
				\end{align*}
				For any constant $c_i$. \\
				Choose $c_i$ so that 
				\begin{align*}
					\Cov((x_{i} - c_i \bar{x})\gamma, \bar{x} \gamma) = 0
				\end{align*} 
				maybe assume normality so that this guarantees independence.
				Then,
				\begin{align*}
					\E((x_i - c_i \bar{x}) \gamma | \bar{x} \gamma = \beta_{ATE}) = (x_{i} - c_i \bar{x}) \E(\gamma)
				\end{align*}
				So then
				\begin{align*}
					(x_i - c_i \bar{x}) \E(\gamma) + c_i \beta_{ATE} = x_i \E(\gamma) + c_i (\beta_{ATE} - \bar{x} \E(\gamma))
				\end{align*} 
				($c_i$ depends on $x_i$)

				In other words, your belief is your prior, adjusted based on the difference between the observed ATE and your prior about the ATE.
				The key question is how much adjustment you do which depends on $"c_i"$. We choose $c_i$ to solve:

				\begin{align*}
                          & \Cov ((x_i - c_i \bar{x}) \gamma, \bar{x} \gamma) = 0                          \\
				 \iff     & \Cov(x_i \gamma, \bar{x} \gamma) -c_i \Cov(\bar{x} \gamma, \bar{x} \gamma) = 0 \\
					\iff  & \Cov(x_i \gamma, \bar{x} \gamma) = c_i \Var(\bar{x} \gamma)                  \\
				\iff & c_i = \frac{\Cov(\beta_i, \beta_{ATE}) }{ \Var(\beta_{ATE})}
				\end{align*} 
				The random variable in this context is $\gamma$ (the coefficients on the $x $'s) in this case $\Var(\beta_{ATE})$ is a measure of how uncertain one was about what $\beta_{ATE}$ would be before doing the trial.

				$c_i$ is the equation for a regression of $\beta_i$ on $\beta_{ATE}$. In other words, we take a bunch of patients with characteristics $x_i$ and we keep redrawing the gammas from our prior distribution the we ask how correlated  $\beta_i$ and $\beta_{ATE}$ are. If they are more correlated (as they would be for patients where the $x_i$ are closer to $\bar{x}$ we update more.

				To compute $c_i$, we just need to know $x_i$ $\bar{x}$, and the distribution of $\gamma$.

				Suppose we want to design the trial to minimize:
				\begin{align*}
					\min \E[(\beta_i - \beta_{i,post})^2]
				\end{align*} 
		\end{enumerate}

\end{enumerate} 

\subsection*{Simple Cases}
\begin{enumerate}
	\item There is just one $x$ and it is binary (old v young). Can it be solved analytically?
	\item Can you solve a 2-dimensional case? 
\end{enumerate}


First observe that in our current setup, we have that
\begin{align*}
	c = \frac{\Cov ( \gamma_0 + \gamma_1 x , \gamma_0  + \gamma_1 \bar{x} )}{ \Var( \gamma_0  + \gamma_1 \bar{x} ))}
\end{align*} 


so for individual i, $c$ reduces to

\begin{align*}
		c &= \frac{\Var(\gamma_0) + (x + \bar{x}) \Cov(\gamma_0, \gamma_1) + x \bar{x} \Var(\gamma_1)}{\Var(\gamma_0) + 2 \bar{x} \Cov(\gamma_0, \gamma_1) + \bar{x}^2\Var(\gamma_1)} \\
		C_{women} &= \frac{\Var(\gamma_0) + \bar{x} \Cov(\gamma_0, \gamma_1)}{\Var(\gamma_0) + 2 \bar{x} \Cov(\gamma_0, \gamma_1) + \bar{x}^2\Var(\gamma_1)} \\
	C_{men} &= \frac{\Var(\gamma_0) + (1 + \bar{x}) \Cov(\gamma_0, \gamma_1) + \bar{x} \Var(\gamma_1)}{\Var(\gamma_0) + 2 \bar{x} \Cov(\gamma_0, \gamma_1) + \bar{x}^2\Var(\gamma_1)} \\
\end{align*} 


\section{Results}


Note, we use $p$ to be the population proportion and $\bar{x}$ to be the average characteristics in the clinical trial. For simplicity, we will call $x = 1$ men, and $x = 0$ women and thus $\bar{x}$ represents the proportion of men in the trial.

\subsection*{Case 1}
For Case 1, we have 

\begin{align*}
	\beta_i &= (1 - x) \gamma_0 + x \gamma_1 \\
	\beta_{ATE} &= (1 - \bar{x}) \gamma_0 + \bar{x} \gamma_1 \\
\end{align*}

For this case we get that whenever $p > \frac{\gamma_0}{ \gamma_0 + \gamma_1}$. When this is true, we only choose men to be in our trial. If $p < \frac{\gamma_0}{ \gamma_0 + \gamma_1}$ we choose all women.
Otherwise, if $p = \frac{\gamma_0}{ \gamma_0 + \gamma_1}$ it doesn't matter what proportions we include in our trial, the MSE will be the same.

\subsection*{Case 2}

For Case 2, we have 

\begin{align*}
	\beta_i &=  \gamma_0 + x \gamma_1 \\
	\beta_{ATE} &=  \gamma_0 + \bar{x} \gamma_1 \\
\end{align*}


For Case $2$, we see that we always have a preference for more men (if we have a population proportion of $p = \frac{1}{2}$, then we would want $\approx 61.8 \%$ of the trial to be men.
This is because they have a higher variance in outcomes. To further investigate the effect of variance in this case, can also observe that as $\Var(\gamma_1) \to \infty$ holding $\Var(\gamma_0)$ fixed we always choose only men.
And as $\Var(\gamma_0) \to \infty$ holding $\Var(\gamma_1)$ fixed, we choose $\bar{x} = p$, equal to the population proportion.

\subsection*{Case 3}

In this case, we consider case 2
\begin{align*}
	\beta_i &=  \gamma_0 + x \gamma_1 \\
	\beta_{ATE} &=  \gamma_0 + \bar{x} \gamma_1 \\
\end{align*}

but allowing for arbitrary correlation betweem $\gamma_0$ and $\gamma_1$.

First, we can see that when we let  $\Cov(\gamma_0, \gamma_1) = 0$, then we recover case 2.
% But if we allow $\Cov(\gamma_0, \gamma_1) = - \Var(\gamma_0)$

Furthermore, our intuition from the previous results suggest that the representation depends on the respective variances in outcomes of each group.

We will thus consider a set of options.

Given the setup, we can observe that
\begin{align*}
	\Var(\beta_{women}) &= \Var(\gamma_0) \\
	\Var(\beta_{men}) &= \Var(\gamma_0) + 2\Cov(\gamma_0, \gamma_1) + \Var(\gamma_1) \\
\end{align*} 

So we will impose that $\Var(\gamma_0) = \Var(\gamma_1) = 1$ and consider different covariances to make men or women have more variance.

From the variances for outcomes above, we can see that for men and women to have equal variance we must impose that  $\Cov(\gamma_0, \gamma_1) = -\frac{1}{2}$.

We investigate different covariance options in figure \ref{fig:cov_options}

\begin{figure}[btp]
	\centering
	\includegraphics[width=0.8\textwidth]{cov_options}
	\caption{The figures contain different options for $\Cov(\gamma_0, \gamma_1)$ and for all but the last graph, $\Var(\gamma_0) = \Var(\gamma_1) = 1$} 
	\label{fig:cov_options}
\end{figure}

\subsection*{Case 4}

In this case, we do the same thing as case 3 but apply it to generalize case 1. recall in case 1 we had
\begin{align*}
	\beta_i &=  (1-x)\gamma_0 + x \gamma_1 \\
	\beta_{ATE} &=  (1 - \bar{x})\gamma_0 + \bar{x} \gamma_1 \\
\end{align*}

but allowing for arbitrary correlation betweem $\gamma_0$ and $\gamma_1$.

First, we can see that when we let  $\Cov(\gamma_0, \gamma_1) = 0$, then we recover case 1.
% But if we allow $\Cov(\gamma_0, \gamma_1) = - \Var(\gamma_0)$

Furthermore, our intuition from the previous results suggest that the representation depends on the respective variances in outcomes of each group.

We will thus consider a set of options.

Given the setup, we can observe that
\begin{align*}
	\Var(\beta_{women}) &= \Var(\gamma_0) \\
	\Var(\beta_{men}) &= \Var(\gamma_1)
\end{align*} 

And in this setup, the covariance structure doesn't tell us which group has more variance, but it tells us how much learning about men tells us about the distribution of results for women and vice versa. 
With this interpretation, we can see that as $\Cov(\gamma_0, \gamma_1) \to 1$ the optimal study proportions $\bar{x} \to p$.

Likewise, as  $\Cov(\gamma_0, \gamma_1) \to 0$, we recover the results from case 1 where the equal variances tell us that because no group has greater variance, whichever group is more common in the true population should be maximized in the study.  e.g. if $p >  \frac{1}{2}$ then $\bar{x} = 1$ and vice versa.

Finally, if $\Cov(\gamma_0, \gamma_1) \in (-1,0)$ then we get edge cases where if $p > \frac{1}{2}$ then MSE is minimized at $\bar{x} = 1$ and vice versa. If $p=\frac{1}{2}$ then $\bar{x} \in \{0,1\}$ are both solutions. And as $\Cov(\gamma_0, \gamma_1) \to -1$, MSE converges pointwise to 0 with a discontinuity at $\frac{1}{2}$.

We can see these results in figure \ref{fig:generalized_case1}

\begin{figure}[ht!]
  \centering
	\includegraphics[width=0.8\textwidth]{generalized_case1}
  % \caption{Plot of $\frac{\partial J}{\partial \bar{x}}$}
	\caption{Here we show the optimal trial proportion of men vs the population proprtion (on the y and x axis respectively) for equal variances of 1.} 
	\label{fig:generalized_case1}
\end{figure}


\section{Appendix}

\begin{center}
	{\large \bf Derivations }
\end{center}


\input{case-1-derivation.tex}
\input{case-2-derivation.tex}
\input{case-3-derivation.tex}
\input{case-4-derivation.tex}
\input{case-5-derivation.tex}


\end{document}

